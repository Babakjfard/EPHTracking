group_by(AnalyteCode, Year, AggregationType) %>%
summarise(MaxAnnualMean= round(max(Concentration), 2)) %>% select(c(AnalyteCode, Year, MaxAnnualMean)) %>%
pivot_wider(names_from = Year, values_from = MaxAnnualMean)
write_csv(check_8_mean, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Annual_max_of_means.csv')
# Check if every Analyte has both annual max and annual mean (HTG, Appendix F - number 8)
# table --> c('AnalyteCode', 'SummaryTimePeriod', 'Year', 'AggregationType')
check_8_max <- Whole_Summaries %>% filter(str_length(SummaryTimePeriod)==4, AggregationType=='MX') %>%
group_by(AnalyteCode, Year, AggregationType) %>%
summarise(MaxAnnualMean= round(max(Concentration), 2)) %>% select(c(AnalyteCode, Year, MaxAnnualMean)) %>%
pivot_wider(names_from = Year, values_from = MaxAnnualMean)
write_csv(check_8_max, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Annual_max_of_max.csv')
View(check_8_max)
View(check_8_mean)
View(check_8_max)
View(check_8_mean)
View(check_8_max)
View(check_8_mean)
View(check_8_max)
sampling <- read_csv('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Sample_results.csv')
# Checking the assigned ConcentrationUnits follow the rules in the dictionary
unique(sampling[c('AnalyteCode', 'ConcentrationUnits')]) #TODO: Add this to the validation class
# HTG I.b.3: 3.	For Uranium samples that are reported in pCi/L, convert pCi to µg
# using the following conversion: 0.67 pCi/µg  (or pCi/L x 1.49 = µg/L)
# TODO: Move it into a callable function
sampling[sampling$AnalyteCode==4010, ] <- sampling %>% filter(AnalyteCode==4010) %>%
mutate(Concentration = Concentration*1.49, ConcentrationUnits='ug/l')
# Flag the concentration ==0 as noDetect
sampling$noDetect <- as.integer(sampling$Concentration==0)
# Changing 0 concentrations (non-detects) into Half LDL (App. G HTG)
replace_concentration <- function(df) {
df %>%
mutate(Concentration = case_when(
AnalyteCode == 1005 & Concentration == 0 ~ 0.01,
AnalyteCode == 2050 & Concentration == 0 ~ 0.002,
AnalyteCode == 2456 & Concentration == 0 ~ 0.25,
AnalyteCode == 2950 & Concentration == 0 ~ 0.005,
AnalyteCode == 2039 & Concentration == 0 ~ 0.23,
AnalyteCode == 1040 & Concentration == 0 ~ 0.001,
AnalyteCode == 2987 & Concentration == 0 ~ 0.005,
AnalyteCode == 2984 & Concentration == 0 ~ 0.001,
AnalyteCode == 4010 & Concentration == 0 ~ 0.02,
AnalyteCode == 4006 & Concentration == 0 ~ 0.0005,
TRUE ~ Concentration
))
}
sampling <- replace_concentration(sampling)
# On HTG: II.b.1:
## Annual mean --> ??: Should I use 'Annual' as TimePeriodType, or follow the Dictionary?
# To calculate :
## Annual Averages in two steps
# average concentration values are derived from first averaging by sampling station, then averaging by CWS
annual_mean_station_8 <- sampling %>% filter(AnalyteCode %notin% c(2456, 2950)) %>%
group_by(PWSIDNumber, Year, AnalyteCode, SamplePointID) %>%
summarise(DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= length(unique(SamplePointID)),
SummaryTimePeriod=unique(Year), NumSamples =n(), NumNonDetects = sum(noDetect))
annual_summary_mean_8 <- annual_mean_station_8 %>% group_by(PWSIDNumber,Year, AnalyteCode) %>%
summarise(DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = as.character(unique(Year)),
NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))
# For disinfection-by-products (TTHM: 2950 and HAA5:2456) annual and quarterly average concentration values are
# derived from first averaging by day, then by CWS
annual_mean_disinfection <- sampling %>% filter(AnalyteCode %in% c(2456, 2950)) %>%
group_by(PWSIDNumber, Year, AnalyteCode, DateSampled) %>%
summarise(AggregationType ='X', Concentration=mean(Concentration), ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= length(unique(SamplePointID)), SummaryTimePeriod = unique(Year),
NumSamples =n(), NumNonDetects = sum(noDetect))
annual_summary_mean_disinfection <- annual_mean_disinfection %>% group_by(PWSIDNumber,Year, AnalyteCode) %>%
summarise(DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = as.character(unique(Year)),
NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))
# Maximums for all 10 analytes are derived by taking the annual maximum for each CWS
annual_summary_max <- sampling %>% group_by(PWSIDNumber,Year, AnalyteCode) %>%
summarise(DateSampled=max(DateSampled), AggregationType ='MX', Concentration=max(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= length(unique(SamplePointID)),
SummaryTimePeriod = as.character(unique(Year)),NumSamples =n(), NumNonDetects = sum(noDetect))
# ============= Quarterly data
# mean (“X”) quarterly data for atrazine (2050), nitrate (1040), HAA5 (2456) and TTHM (2950)
library(tidyverse)
library(lubridate)
extract_Quarters <- function(theDataframe){
theDataframe %>%
mutate(
theYear = year(DateSampled),
Quarter = case_when(
month(DateSampled) %in% 1:3 ~ "Q1",
month(DateSampled) %in% 4:6 ~ "Q2",
month(DateSampled) %in% 7:9 ~ "Q3",
month(DateSampled) %in% 10:12 ~ "Q4"
),
SummaryTimePeriod = paste(theYear, Quarter, sep="-")
) %>%
select(-c(theYear, Quarter))
}
# ============
sampling_Q <- sampling %>% filter(AnalyteCode %in% c(2050, 1040, 2456, 2950))
sampling_Q <- extract_Quarters(sampling_Q)
extract_Quarters <- function(theDataframe){
theDataframe %>%
mutate(
theYear = year(DateSampled),
Quarter = case_when(
month(DateSampled) %in% 1:3 ~ "1",
month(DateSampled) %in% 4:6 ~ "2",
month(DateSampled) %in% 7:9 ~ "3",
month(DateSampled) %in% 10:12 ~ "4"
),
SummaryTimePeriod = paste(theYear, Quarter, sep="-")
) %>%
select(-c(theYear, Quarter))
}
# just checking
#sum(sampling_Q$Year != sampling_Q$theYear)
# ====== Quarterly averages for atrazine (2050), nitrate (1040)
quarterly_mean_atr_nitr <- sampling_Q %>% filter(AnalyteCode %in% c(2050, 1040)) %>%
group_by(PWSIDNumber, SummaryTimePeriod, AnalyteCode, SamplePointID) %>%
summarise(DateSampled=max(DateSampled), Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= length(unique(SamplePointID)),
SummaryTimePeriod=unique(SummaryTimePeriod), NumSamples =n(), NumNonDetects = sum(noDetect))
quarterly_summary_atr_nitr <- quarterly_mean_atr_nitr %>% group_by(PWSIDNumber,SummaryTimePeriod, AnalyteCode) %>%
summarise(DateSampled=max(DateSampled), Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = unique(SummaryTimePeriod),
NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))
# ====== Now for disinfection-by-products HAA5 (2456) and TTHM (2950)
quarterly_mean_disinfection <- sampling_Q %>% filter(AnalyteCode %in% c(2456, 2950)) %>%
group_by(PWSIDNumber, SummaryTimePeriod, AnalyteCode, DateSampled) %>%
summarise(Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration), ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= length(unique(SamplePointID)), SummaryTimePeriod = unique(SummaryTimePeriod),
NumSamples =n(), NumNonDetects = sum(noDetect))
quarterly_summary_disinfection <- quarterly_mean_disinfection %>% group_by(PWSIDNumber,SummaryTimePeriod, AnalyteCode) %>%
summarise(Year= unique(Year), DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = unique(SummaryTimePeriod),
NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))
View(quarterly_mean_atr_nitr)
extract_Quarters <- function(theDataframe){
theDataframe %>%
mutate(
theYear = year(DateSampled),
Quarter = case_when(
month(DateSampled) %in% 1:3 ~ "1",
month(DateSampled) %in% 4:6 ~ "2",
month(DateSampled) %in% 7:9 ~ "3",
month(DateSampled) %in% 10:12 ~ "4"
),
SummaryTimePeriod = paste(theYear, Quarter, sep="-")
) %>%
select(-c(theYear, Quarter))
}
# ============
sampling_Q <- sampling %>% filter(AnalyteCode %in% c(2050, 1040, 2456, 2950))
sampling_Q <- extract_Quarters(sampling_Q)
# just checking
#sum(sampling_Q$Year != sampling_Q$theYear)
# ====== Quarterly averages for atrazine (2050), nitrate (1040)
quarterly_mean_atr_nitr <- sampling_Q %>% filter(AnalyteCode %in% c(2050, 1040)) %>%
group_by(PWSIDNumber, SummaryTimePeriod, AnalyteCode, SamplePointID) %>%
summarise(DateSampled=max(DateSampled), Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= length(unique(SamplePointID)),
SummaryTimePeriod=unique(SummaryTimePeriod), NumSamples =n(), NumNonDetects = sum(noDetect))
quarterly_summary_atr_nitr <- quarterly_mean_atr_nitr %>% group_by(PWSIDNumber,SummaryTimePeriod, AnalyteCode) %>%
summarise(DateSampled=max(DateSampled), Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = unique(SummaryTimePeriod),
NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))
View(quarterly_summary_atr_nitr)
# ====== Now for disinfection-by-products HAA5 (2456) and TTHM (2950)
quarterly_mean_disinfection <- sampling_Q %>% filter(AnalyteCode %in% c(2456, 2950)) %>%
group_by(PWSIDNumber, SummaryTimePeriod, AnalyteCode, DateSampled) %>%
summarise(Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration), ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= length(unique(SamplePointID)), SummaryTimePeriod = unique(SummaryTimePeriod),
NumSamples =n(), NumNonDetects = sum(noDetect))
quarterly_summary_disinfection <- quarterly_mean_disinfection %>% group_by(PWSIDNumber,SummaryTimePeriod, AnalyteCode) %>%
summarise(Year= unique(Year), DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
ConcentrationUnits=unique(ConcentrationUnits),
NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = unique(SummaryTimePeriod),
NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))
View(quarterly_summary_atr_nitr)
View(quarterly_mean_atr_nitr)
View(quarterly_summary_atr_nitr)
View(quarterly_summary_disinfection)
# Now mixing all summary data (5 tables) into one table to easily check it
# annual_summary_max, annual_summary_mean_8, annual_summary_mean_disinfection,
# quarterly_summary_atr_nitr, quarterly_summary_disinfection
Whole_Summaries <- bind_rows(annual_summary_max, annual_summary_mean_8,
annual_summary_mean_disinfection, quarterly_summary_atr_nitr,
quarterly_summary_disinfection)
col_order <- c("PWSIDNumber", "Year", "AnalyteCode", "ConcentrationUnits", "Concentration",
"DateSampled", "AggregationType", "NumSamplingLocations",
"SummaryTimePeriod", "NumSamples", "NumNonDetects" )
Whole_Summaries <- Whole_Summaries[, col_order]
write_csv(Whole_Summaries, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Summaries_Calculated_20230407.csv')
# ==============compare my summaries with theirs
their_summary <- read_csv('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSSampleSummary_latest.csv')
View(their_summary)
#??? Do Measures (First Section in How-To Guide)
colnames(their_summary)
colnames(Whole_Summaries)
# The very first big difference!
dim(Whole_Summaries)[1] - dim(their_summary)[1]
head(Whole_Summaries)
View(their_summary)
to_compare <- Whole_Summaries %>% select(-c(Year, DateSampled, ConcentrationUnits))
names(to_compare)
names(their_summary)
to_compare <- Whole_Summaries %>% select(-c(Year, DateSampled, ConcentrationUnits))
to_compare <- Whole_Summaries %>% ungroup() %>%
select(-c(Year, DateSampled, ConcentrationUnits))
# Merge the two
merged <- merge(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"))
View(merged)
colnames(their_summary)
colnames(Whole_Summaries)
their_summary <- their_summary %>% select(-c(AnalyteName))
to_compare <- Whole_Summaries %>% ungroup() %>%
select(-c(Year, DateSampled, ConcentrationUnits, NumNonDetects))
# Merge the two
merged <- merge(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"), all=TRUE)
View(merged)
colSums(is.na(their_summary))
# How many Na they have?
colSums(is.na(their_summary))
colSums(is.na(Whole_Summaries))
write_csv(merged, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/comparisons_us_NDEE.csv')
unique(their_summary$AnalyteCode)
unique(Whole_Summaries$AnalyteCode)
their_summary <- their_summary %>%
mutate(AnalyteCode = if_else(AnalyteCode == 1038, 1040, AnalyteCode))
unique(their_summary$AnalyteCode)
their_summary <- their_summary %>% filter(AnalyteCode != 1041)
# How many Na they have?
colSums(is.na(their_summary)) #53 have no aggregation type
to_compare <- Whole_Summaries %>% ungroup() %>%
select(-c(Year, DateSampled, ConcentrationUnits, NumNonDetects))
# Merge the two
merged <- merge(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"), all=TRUE)
# Merge the two
merged <- merge(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"))
unique(their_summary$AnalyteCode)
unique(to_compare$AnalyteCode)
# Merge the two
merged <- merge(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"), all = TRUE)
View(merged)
colSums(is.na(merged))
write_csv(merged, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/comparisons_us_NDEE.csv')
# What are those extras that we have but they don't
we_have_they_dont <- anti_join(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"))
View(we_have_they_dont)
write_csv(we_have_they_dont, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/we_have_they_dont.csv')
# What are those extras that they have but we don't
they_have_we_dont <- anti_join(their_summary, to_compare, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"))
View(they_have_we_dont)
write_csv(they_have_we_dont, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/they_have_we_dont.csv')
colSums(is.na(their_summary))
their_summary[is.na(their_summary),]
their_summary[is.na(their_summary$AggregationType),]
print(their_summary[is.na(their_summary$AggregationType),])
test <- their_summary[is.na(their_summary$AggregationType),]
View(test)
their_summary[is.na(their_summary$AggregationType),'AggregationType'] <- 'X'
colSums(is.na(their_summary))
# ====================================================
# ==============compare my summaries with theirs ======
their_summary <- read_csv('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSSampleSummary_latest.csv')
their_summary <- their_summary %>% select(-c(AnalyteName))
their_summary <- their_summary %>%
mutate(AnalyteCode = if_else(AnalyteCode == 1038, 1040, AnalyteCode))
their_summary <- their_summary %>% filter(AnalyteCode != 1041)
# How many Na they have?
colSums(is.na(their_summary)) #53 have no aggregation type
their_summary[is.na(their_summary$AggregationType),'AggregationType'] <- 'X'
to_compare <- Whole_Summaries %>% ungroup() %>%
select(-c(Year, DateSampled, ConcentrationUnits, NumNonDetects))
# Merge the two
merged <- merge(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"), all = TRUE)
write_csv(merged, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/comparisons_us_NDEE.csv')
# What are those extras that we have but they don't
we_have_they_dont <- anti_join(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"))
write_csv(we_have_they_dont, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/we_have_they_dont.csv')
# What are those extras that they have but we don't
they_have_we_dont <- anti_join(their_summary, to_compare, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"))
write_csv(they_have_we_dont, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/they_have_we_dont.csv')
View(they_have_we_dont)
Whole_Summaries %>% filter(AnalyteCode==1005)
Whole_Summaries %>% filter(PWSIDNumber=='NE3100101', Year==2012, AnalyteCode==1005)
unique(we_have_they_dont$AnalyteCode)
sampling %>% filter(PWSIDNumber=='NE3100101', Year==2012, AnalyteCode==1005)
sampling %>% filter(PWSIDNumber=='NE3100101', Year==2012, AnalyteCode==1005)
unique(they_have_we_dont$AnalyteCode)
write_csv(merged, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/comparisons_us_NDEE_V2.csv')
# What are those extras that we have but they don't
we_have_they_dont <- anti_join(to_compare, their_summary, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"))
write_csv(we_have_they_dont, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/we_have_they_dont_V2.csv')
# What are those extras that they have but we don't
they_have_we_dont <- anti_join(their_summary, to_compare, by=c("PWSIDNumber","AnalyteCode",
"AggregationType", "SummaryTimePeriod"))
write_csv(they_have_we_dont, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/they_have_we_dont_V2.csv')
View(their_summary)
# ====================================================
# ==============compare my summaries with theirs ======
their_summary <- read_csv('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSSampleSummary_latest.csv')
their_summary <- their_summary %>% select(-c(AnalyteName))
their_summary <- their_summary %>%
mutate(AnalyteCode = if_else(AnalyteCode == 1038, 1040, AnalyteCode))
their_summary <- their_summary %>% filter(AnalyteCode != 1041)
# How many Na they have?
colSums(is.na(their_summary)) #53 have no aggregation type
their_summary[is.na(their_summary$AggregationType),'AggregationType'] <- 'X'
unique(their_summary$SummaryTimePeriod)
their_summary %>% filter(str_length(SummaryTimePeriod)>4) %>% summarise(Analytis=unique(AnalyteCode))
Whole_Summaries %>% filter(str_length(SummaryTimePeriod)>4) %>% summarise(Analytis=unique(AnalyteCode))
their_summary %>% filter(str_length(SummaryTimePeriod)>4) %>% summarise(Analytis=unique(AnalyteCode))
Whole_Summaries %>% ungroup()%>% filter(str_length(SummaryTimePeriod)>4) %>% summarise(Analytis=unique(AnalyteCode))
library(tidyverse)
`%notin%` <- Negate(`%in%`)
sampling <- read_csv('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Sample_results.csv')
# Checking the assigned ConcentrationUnits follow the rules in the dictionary
unique(sampling[c('AnalyteCode', 'ConcentrationUnits')]) #TODO: Add this to the validation class
# HTG I.b.3: 3.	For Uranium samples that are reported in pCi/L, convert pCi to µg
# using the following conversion: 0.67 pCi/µg  (or pCi/L x 1.49 = µg/L)
# TODO: Move it into a callable function
sampling[sampling$AnalyteCode==4010, ] <- sampling %>% filter(AnalyteCode==4010) %>%
mutate(Concentration = Concentration*1.49, ConcentrationUnits='ug/l')
# ***# Named vector with LDL values - Appendix G in HTG 2022
## ???: Should this check be done before conversation of Uranium or after?
ldl_vec <- c(`1005` = 0.02, `2050` = 0.003, `2456` = 0.5, `2950` = 0.01, `2039` = 0.46,
`1040` = 0.002, `2987` = 0.01, `2984` = 0.002, `4010` = 0.03, `4006` = 0.001)
LDL_check <- function(df, ldl_dict) {
df %>%
mutate(LD_Flag = ifelse(Concentration < ldl_dict[as.character(AnalyteCode)], 1, 0))
}
sampling <- LDL_check(sampling, ldl_vec)
sum(sampling$LD_Flag)
View(sampling)
test <- sampling %>% filter(LD_Flag ==1)
View(test)
sum(test$Concentration)
recent_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSSampleSummaryLead_3_30_23.xlsx')
second_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PB90Summary_3_29_23.xlsx')
# 2) Time period is 2018 to 2020 (Three years)
unique(recent_file$Year)
unique(second_file$Year)
required_time_span <- 2018:2020
recent_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSSampleSummaryLead_3_30_23.xlsx') %>%
filter(Year %in% required_time_span)
# Comparing the two lead files
library(tidyverse)
required_time_span <- 2018:2020
recent_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSSampleSummaryLead_3_30_23.xlsx') %>%
filter(Year %in% required_time_span)
second_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PB90Summary_3_29_23.xlsx') %>%
filter(Year %in% required_time_span)
# 2) Time period is 2018 to 2020 (Three years)
unique(recent_file$Year)
unique(second_file$Year)
head(first_file)
recent_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSSampleSummaryLead_3_30_23.xlsx') %>%
filter(Year %in% required_time_span)
second_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PB90Summary_3_29_23.xlsx') %>%
filter(Year %in% required_time_span)
older_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PB90Summary_3_29_23.xlsx') %>%
filter(Year %in% required_time_span)
head(recent_file)
head(older_file)
names(recent_file)
names(older_file)
a <- anti_join(recent_file, older_file, by=c("PWSIDNumber", "Year", "DateSampled"))
a <- anti_join(older_file, recent_file,  by=c("PWSIDNumber", "Year", "DateSampled"))
recent_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSSampleSummaryLead_3_30_23.xlsx') %>%
filter(Year %in% required_time_span)
older_file <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PB90Summary_3_29_23.xlsx') %>%
filter(Year %in% required_time_span)
View(older_file)
View(older_file)
# Getting read of duplicates
older <- older() %>%
distinct()
# Getting read of duplicates
older_file <- older_file %>%
distinct()
View(older_file)
recent_file <- recent_file %>%
distinct()
(2.140+3.90)/2
View(recent_file)
names(rece]) - names(older_file)
names(rece]) %in% names(older_file)
names(recent_file) - names(older_file)
names(recent_file) %in% names(older_file)
names(older_file) %in% names(recent_file)
names(recent_file)
union()
# Let's join the two to make comparison easier
a <- merge(older_file, recent_file, by=c('PWSIDNumber', 'Year', 'DateSampled'),
suffixes = c('_old', '_rec'), all = TRUE)
write_csv(a, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Merged_Lead.csv')
names(a)
names(older_file)
merged <- a[,c("PWSIDNumber", "Year", "DateSampled", "SummaryTimePeriod_old", "SummaryTimePeriod_rec",
"AnalyteCode_old", "AnalyteCode_rec", "ConcentrationUnits_old", "ConcentrationUnits_rec",
"Concentration_old", "Concentration_rec", "AggregationType_old","AggregationType_rec",
"NumSamples_old", "NumSamples_rec", )]
merged <- a[,c("PWSIDNumber", "Year", "DateSampled", "SummaryTimePeriod_old", "SummaryTimePeriod_rec",
"AnalyteCode_old", "AnalyteCode_rec", "ConcentrationUnits_old", "ConcentrationUnits_rec",
"Concentration_old", "Concentration_rec", "AggregationType_old","AggregationType_rec",
"NumSamples_old", "NumSamples_rec")]
write_csv(merged, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Merged_Lead.csv')
sum(year(older_file$DateSampled)-older_file$Year)
library(lubridate)
sum(year(older_file$DateSampled)-older_file$Year)
sum(year(recent_file$DateSampled)-recent_file$Year)
# Check duplicate rows in the older one
older_dups <- older_file %>% group_by(PWSIDNumber, DateSampled) %>%
summarise(duplicates=n(), .groups = 'drop')
# Check duplicate rows in the older one
older_dups <- older_file %>% group_by(PWSIDNumber, DateSampled) %>%
summarise(duplicates=n(), .groups = 'drop') %>% ungroup()
# Check duplicate rows in the older one
older_dups <- older_file %>% group_by(PWSIDNumber, DateSampled) %>%
summarise(duplicates=n(), .groups = 'drop') %>%
left_join(older_file, by=c('PWSIDNumber', 'DateSampled'))
View(older_dups)
recent_dups <- recent_file %>% group_by(PWSIDNumber, DateSampled) %>%
summarise(duplicates=n(), .groups = 'drop') %>%
left_join(recent_file, by=c('PWSIDNumber', 'DateSampled'))
View(recent_dups)
older_dups <- older_file %>% group_by(PWSIDNumber, DateSampled) %>%
summarise(duplicates=n(), .groups = 'drop')
# Let's averagew duplicated ones in each file and see if it reaches the same result
older_ave <- older_file %>% group_by(PWSIDNumber, DateSampled) %>%
summarise(Concentration = mean(Concentration), .groups = 'drop')
recent_ave <- recent_file %>% group_by(PWSIDNumber, DateSampled) %>%
summarise(Concentration = mean(Concentration), .groups = 'drop')
test <- merge(recent_ave, older_ave, by= c('PWSIDNumber', 'DateSampled'))
test$Concentration.x - test$Concentration.y
test <- merge(recent_ave, older_ave, by= c('PWSIDNumber', 'DateSampled'),
suffixes = c('_rec', '_old'))
test$Concentration_rec-test$Concentration_old
write(test, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/uniqued_values_lead.csv' )
write_csv(test, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/uniqued_values_lead.csv' )
inventories <- read_csv('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSInventory_latest.csv')
`%notin%` <- Negate(`%in%`)
sum(test$PWSIDNumber %notin% inventories$PWSIDNumber)
sum(inventories$PWSIDNumber %notin% test$PWSIDNumber)
lead_inventories <- unique(recent_file$PWSIDNumber)
general_inventories <- unique(inventories$PWSIDNumber)
lead_inventories <- NULL
general_inventories<- NULL
inventories_general <- unique(inventories$PWSIDNumber)
inventories_lead <- unique(recent_file$PWSIDNumber)
sum(inventories_lead %notin% inventories_general)
inventories_missing <- inventories_lead[inventories_lead %notin% inventories_general]
inventories_missing <- data_frame(missing=inventories_lead[inventories_lead %notin% inventories_general])
inventories_missing <- tibble(missing=inventories_lead[inventories_lead %notin% inventories_general])
write(inventories_missing, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/inventories_missing.csv')
View(inventories_missing)
write(inventories_missing, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/inventories_missing.csv')
inventories_missing
write(inventories_missing, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/inventories_missing.csv')
inventories_missing <- tibble(missing=unlist(inventories_lead[inventories_lead %notin% inventories_general]))
write(inventories_missing, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/inventories_missing.csv')
inventories_missing <- tibble(missing=as.character(unlist(inventories_lead[inventories_lead %notin% inventories_general])))
write(inventories_missing, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/inventories_missing.csv')
inventories_missing <- data.frame(missing=inventories_lead[inventories_lead %notin% inventories_general])
write(inventories_missing, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/inventories_missing.csv')
inventories_missing <- tibble(missing=inventories_lead[inventories_lead %notin% inventories_general])
write_csv(inventories_missing, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/inventories_missing.csv')
View(inventories_missing)
library(sf)
inventory <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSInventory.xlsx')
names(inventories)
pws_sf <- st_as_sf(inventory, coords = c("Longitude", "Latitude"))
names(inventory)
inventory <- inventory %>% rename(Latitude=MinOfLatitude, Longitude=MinOfLongitude)
PWS_sf <- st_as_sf(inventory, coords = c("Longitude", "Latitude"))
st_crs(PWS_sf) <- 4269
st_write(PWS_sf, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSInventory.shap')
st_write(PWS_sf, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSInventory.shp')
inventory <- readxl::read_xlsx('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSInventory.xlsx')
inventory <- inventory %>% rename(Latitude=MinOfLatitude, Longitude=MinOfLongitude, PrincipalCityFeatureId = \ufeffPrincipalCityFeatureId)
inventory$`﻿PrincipalCityFeatureId`
inventory <- inventory %>% rename(Latitude=MinOfLatitude, Longitude=MinOfLongitude, PrincipalCityFeatureId = `﻿PrincipalCityFeatureId`)
inventory$PrincipalCityFeatureId
PWS_sf <- st_as_sf(inventory, coords = c("Longitude", "Latitude"))
st_crs(PWS_sf) <- 4269
st_write(PWS_sf, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSInventory.shp')
st_write(PWS_sf, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/PWSInventory.shp')
# Get latest quality check file
samplings <- read_csv('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Summaries_Calculated_20230414.csv')
samplings$PWSIDNumber %notin% inventory$PWSIDNumber
samplings[samplings$PWSIDNumber %notin% inventory$PWSIDNumber, 'PWSIDNumber']
inventory[inventory$PWSIDNumber %notin% samplings$PWSIDNumber, 'PWSIDNumber']
length(unique(samplings$PWSIDNumber))
length(unique(inventory$PWSIDNumber))
