---
title: "Water Date Preparation for CDC Data call Spring 2023"
output:
  word_document: default
  html_notebook: default
---

# Start
This is to document the processing steps in preparing the summary water data for submission to spring 2023 Data Call. This follows the How-to-Guide and Data Dictionary 2023. To get undrestanding about names and/or rules please refer to those documents.
The process was started with the unaggregated file(.xlsx) received from NDEE on 4/4/2023. That contained 68,108 rows of data with the following columns:

`['PWSIDNumber', 'Year', 'AnalyteName', 'AnalyteCode',
       'ConcentrationUnits', 'Concentration', 'DateSampled', 'SamplePointID',
       'DetectionLimit', 'DetectionLimitUom', 'NonDetectFlag']`

It contains sampling results from 2012 to 2021 as below:

Year    Rows of data
------ --------------
2012     9069
2018     7491
2015     7140
2013     6970
2016     6460
2021     6444
2019     6396
2017     6292
2020     5972
2014     5869 

* **Step 1-** Adjusting AnalyteCode
  following the instructions all code values of 1038 were changed into 1040 (21,928 rows). Also all data for Analytedoe 1041 removed, since 
  not required by data call (17 rows).
* **Step 2 -** Initial validation test
  An initial validation against Data Dictionary rules done on columns that will be present in the final summary file. all passed validation for the following columns:
  'RowIdentifier', 'PWSIDNumber', 'Year', 'AnalyteCode',
       'ConcentrationUnits', 'Concentration', 'DateSampled']
       
After these intial steps number of data points bsed on each AnalyteCode are as below:

AnalyteCode   No. Rows
----------- -----------
1040            21945
2050             7684
2039             7684
2987             6518
2984             6518
1005             6268
2950             3457
2456             3416
4010             3093
4006             1503

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

`%notin%` <- Negate(`%in%`)
sampling <- read_csv('/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Sample_results.csv')

```
* **Step 3 -** Checked if the ConcentrationUnits for each AnalyteCode applies with the values in the Data Dictionary
```{r, echo=FALSE, message=FALSE, warning=FALSE}
units_check <- unique(sampling[c('AnalyteCode', 'ConcentrationUnits')])

```

* **Step 4 -** Checked if uranium needs a change of ConcentrationUnits from pci/l to ug/l
```{r, echo=FALSE, message=FALSE, warning=FALSE}
if (units_check[units_check$AnalyteCode==4006, "ConcentrationUnits"] == 'pci/l'){

  sampling[sampling$AnalyteCode==4006, ] <- sampling %>% filter(AnalyteCode==4006) %>%
  mutate(Concentration = Concentration*1.49, ConcentrationUnits='ug/l')
}
```

* **Step 4 -** Checking rows with NonDetectFlag. Below is the result for checking rows with a NonDetectionFlag = 1. Most of them have a half LDL value for their concentrations (refer to HTG Guide), and 74 have no value for Concentration LDL. 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
sampling %>% filter(NonDetectFlag==1) %>% 
  mutate(is_half_LDL= Concentration / DetectionLimit) %>%
  group_by(is_half_LDL) %>%
  summarise(number=n())

```

We checked for the applied LDL and since some of them do not match to the values in HTG, we cahnged all the concentrations for these rows (31,886) into 0, so in the next step will be checked and changed into appropriate half LDL as suggested by HTG 2023.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
sampling[sampling$NonDetectFlag==1, "Concentration"] <- 0.0

# Also set the flag for Concentration == 0
sampling[sampling$Concentration == 0.0 , "NonDetectFlag"] <- 1
```

* **Step 5 -** Followed Appendix G of HTG 2023, checked the Concentration values against thir Lowest Detection Limit (LDL) and replaced with the suggested values where lower

```{r, echo=FALSE, message=FALSE, warning=FALSE}
replace_concentration <- function(df) {
  df_new <- df %>%
    mutate(Concentration = case_when(
      AnalyteCode == 1005 & Concentration < 0.02  ~ 0.01,
      AnalyteCode == 2050 & Concentration < 0.003 ~ 0.002,
      AnalyteCode == 2456 & Concentration < 0.5   ~ 0.25,
      AnalyteCode == 2950 & Concentration < 0.01  ~ 0.005,
      AnalyteCode == 2039 & Concentration < 0.46  ~ 0.23,
      AnalyteCode == 1040 & Concentration < 0.002 ~ 0.001,
      AnalyteCode == 2987 & Concentration < 0.01  ~ 0.005,
      AnalyteCode == 2984 & Concentration < 0.002 ~ 0.001,
      AnalyteCode == 4010 & Concentration < 0.03  ~ 0.02,
      AnalyteCode == 4006 & Concentration < 0.001 ~ 0.0005,
      TRUE ~ Concentration
    ))
  
  num_rows_changed <- sum(df_new$Concentration != df$Concentration)
  
  return(list(df_new, num_rows_changed))
}


# Call the function and assign returned values to variables
result <- replace_concentration(sampling)
sampling <- result[[1]]
num_rows_changed <- result[[2]]

print(paste('number of revised below LDL value:', num_rows_changed ))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
percent_no_detect <- sampling %>% group_by(AnalyteCode)%>% 
  summarise(Total= n(), percent_no_detect=round((sum(NonDetectFlag)+
                                                               sum(Concentration==0))*100/n(),2))
```
* **Step 5 -** Averaged duplicates into one values. Checked for duplicated of same analyte sampled in the same day from same point location, and averaged them into one value. It contained repeatitions for values in columns (PWSIDNumber, Year, AnalyteCode, DateSampled, SamplePointID). Added a new column "NumSamples" that can account for these number of samples. (The highest was 28 for one analyte sampling in the same point in the same date)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
sampling_dps <- sampling %>%
  group_by(PWSIDNumber, Year, AnalyteCode, DateSampled, SamplePointID) %>%
  summarize(ConcentrationUnits= unique(ConcentrationUnits), NumSamples=n(), NumNonDetects=sum(NonDetectFlag), 
            Concentration=mean(Concentration), .groups = "drop")

# Check sampling_dps to make sure it doesn't miss anything
print(paste('initial rows in data: ', dim(sampling)[1]))
sampling <- sampling_dps
print(paste('no. of rows after averaging same day/point/analyte: ', dim(sampling)[1]))

sampling_dps <- NA

```


before starting the summarization, we took a look into maximum concentration value for each analyte compared to its median over all dataset (from 2012 to 2021)
```{r, echo=FALSE, message=FALSE, warning=FALSE}
sampling %>% 
  group_by(AnalyteCode) %>% 
  summarise(maximum = max(Concentration),
            median = median(Concentration)) %>% 
  left_join(sampling, by = c("AnalyteCode", "maximum"= "Concentration")) %>% 
  select(AnalyteCode, maximum, median, DateSampled)

```
 ===> The maximum value for analyte 2039 (DEHP) seems too large compared to its median. May be good to double-check. last column above shows the sampling date for the maximum value. Below, we have also provided plots for the distributions:
 
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=8, fig.align='center'}
library(ggplot2)

# Example dataset
# Create accordion plots using ggplot2
ggplot(sampling, aes(x = AnalyteCode, y = Concentration)) +
  geom_violin(lwd=0.5, draw_quantiles = c(0.25, 0.5, 0.75), scale = "width") +
  geom_boxplot(lwd=1, width = 0.3, fill = "white", outlier.shape = NA) +
  facet_wrap(~ AnalyteCode, ncol = 3, scales = "free") +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size = 12, face = "bold"),
        axis.text.x = element_blank()
        )
  

```


 
## Starting the Summarization
From this step we have followed the HTG guide for summarizing the data. We created three scenarios. 
1- Annual averaging for 8 non-disinfecion By-products
2- Annual averaging for the two Disinfection By-products
3- Annual maximum for all 10 analytes
4- Quarterly values for Nitrate (1040) and Atrazine (2050)
5- Quarterly average for the two Disinfection By-products: TTHM(2950) and HAA5(2456)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
## Annual Averages in two steps
# average concentration values are derived from first averaging by sampling station, 
annual_mean_station_8 <- sampling %>% filter(AnalyteCode %notin% c(2456, 2950)) %>%
  group_by(PWSIDNumber, Year, AnalyteCode, SamplePointID) %>%
  summarise(DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
            ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= length(unique(SamplePointID)), 
            SummaryTimePeriod=unique(Year), NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))

# Step 2: then averaging by CWS
annual_summary_mean_8 <- annual_mean_station_8 %>% group_by(PWSIDNumber,Year, AnalyteCode) %>% 
  summarise(DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
            ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = as.character(unique(Year)),
            NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))


```

* **Step S.1 -** Summarized annual means for the 8 anlytes following HTG 2023:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
annual_summary_mean_8 %>% group_by(Year)%>%
  summarise(NuLocations=sum(NumSamplingLocations), NuSamples=sum(NumSamples), 
            Analytes=length(unique(AnalyteCode)), Non_detects=sum(NumNonDetects))

```
* **Step S.2 -** Summarized annual means for the two Disinfection By-products following HTG 2023:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# For disinfection-by-products (TTHM: 2950 and HAA5:2456) annual and quarterly average concentration values are
# derived from first averaging by day, 
annual_mean_disinfection <- sampling %>% filter(AnalyteCode %in% c(2456, 2950)) %>%
  group_by(PWSIDNumber, Year, AnalyteCode, DateSampled) %>%
  summarise(AggregationType ='X', Concentration=mean(Concentration), ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= length(unique(SamplePointID)), SummaryTimePeriod = unique(Year),
            NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))

# then by CWS
annual_summary_mean_disinfection <- annual_mean_disinfection %>% group_by(PWSIDNumber,Year, AnalyteCode) %>%
  summarise(DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
            ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = as.character(unique(Year)),
            NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))

```

..
```{r, echo=FALSE, message=FALSE, warning=FALSE}
annual_summary_mean_disinfection %>% group_by(Year)%>%
  summarise(NuLocations=sum(NumSamplingLocations), NuSamples=sum(NumSamples), 
            Analytes=length(unique(AnalyteCode)), Non_detects=sum(NumNonDetects))


```

* **Step S.3 -** Summarized annual maximums for all the analytes following HTG 2023:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Maximums for all 10 analytes are derived by taking the annual maximum for each CWS
annual_summary_max <- sampling %>% group_by(PWSIDNumber,Year, AnalyteCode) %>% 
  summarise(DateSampled=max(DateSampled), AggregationType ='MX', Concentration=max(Concentration),
            ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= length(unique(SamplePointID)), 
            SummaryTimePeriod = as.character(unique(Year)),NumSamples =sum(NumSamples),
            NumNonDetects = sum(NumNonDetects))

```
...
```{r, echo=FALSE, message=FALSE, warning=FALSE}
annual_summary_max %>% group_by(Year)%>%
  summarise(NuLocations=sum(NumSamplingLocations), NuSamples=sum(NumSamples), 
            Analytes=length(unique(AnalyteCode)), Non_detects=sum(NumNonDetects))


```
* **Step S.4 -** Summarized quarterly means for all the analytes following HTG 2023:
We first separatd the four analytes for which the quarterly values are required. Nitrate and Atrazine and disinfection-by-products (TTHM and HAA5). Then for each group, since the methods are different and similar to their annual averages, we calculated the average quarterly values

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# ============= Quarterly data
# mean (“X”) quarterly data for atrazine (2050), nitrate (1040), HAA5 (2456) and TTHM (2950)
library(tidyverse)
library(lubridate)

extract_Quarters <- function(theDataframe){
  theDataframe %>%
    mutate(
      theYear = year(DateSampled),
      Quarter = case_when(
        month(DateSampled) %in% 1:3 ~ "1",
        month(DateSampled) %in% 4:6 ~ "2",
        month(DateSampled) %in% 7:9 ~ "3",
        month(DateSampled) %in% 10:12 ~ "4"
      ),
      SummaryTimePeriod = paste(theYear, Quarter, sep="-")
    ) %>%
    select(-c(theYear, Quarter))
}

# ============
sampling_Q <- sampling %>% filter(AnalyteCode %in% c(2050, 1040, 2456, 2950))
sampling_Q <- extract_Quarters(sampling_Q)
```
...
* **Step S.4.1 -** For Atrazine and Nitrate
```{r, echo=FALSE, message=FALSE, warning=FALSE}
#sum(sampling_Q$Year != sampling_Q$theYear)
# ====== Quarterly averages for atrazine (2050), nitrate (1040)
quarterly_mean_atr_nitr <- sampling_Q %>% filter(AnalyteCode %in% c(2050, 1040)) %>%
  group_by(PWSIDNumber, SummaryTimePeriod, AnalyteCode, SamplePointID) %>%
  summarise(DateSampled=max(DateSampled), Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration),
            ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= length(unique(SamplePointID)), 
            SummaryTimePeriod=unique(SummaryTimePeriod), NumSamples = sum(NumSamples), 
            NumNonDetects = sum(NumNonDetects))


quarterly_summary_atr_nitr <- quarterly_mean_atr_nitr %>% group_by(PWSIDNumber,SummaryTimePeriod, AnalyteCode) %>% 
  summarise(DateSampled=max(DateSampled), Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration),
            ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = unique(SummaryTimePeriod),
            NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))

```

...
```{r, echo=FALSE, message=FALSE, warning=FALSE}
quarterly_summary_atr_nitr %>% group_by(SummaryTimePeriod)%>%
  summarise(NuLocations=sum(NumSamplingLocations), NuSamples=sum(NumSamples), 
            Analytes=length(unique(AnalyteCode)), Non_detects=sum(NumNonDetects))


```
Here we only provided the first 10 rows of total 40.


* **Step S.4.1 -** For disinfection by-products
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# ====== Now for disinfection-by-products HAA5 (2456) and TTHM (2950)
quarterly_mean_disinfection <- sampling_Q %>% filter(AnalyteCode %in% c(2456, 2950)) %>%
  group_by(PWSIDNumber, SummaryTimePeriod, AnalyteCode, DateSampled) %>%
  summarise(Year= unique(Year), AggregationType ='X', Concentration=mean(Concentration), ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= length(unique(SamplePointID)), SummaryTimePeriod = unique(SummaryTimePeriod),
            NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))

quarterly_summary_disinfection <- quarterly_mean_disinfection %>% group_by(PWSIDNumber,SummaryTimePeriod, AnalyteCode) %>%
  summarise(Year= unique(Year), DateSampled=max(DateSampled), AggregationType ='X', Concentration=mean(Concentration),
            ConcentrationUnits=unique(ConcentrationUnits),
            NumSamplingLocations= sum(NumSamplingLocations), SummaryTimePeriod = unique(SummaryTimePeriod),
            NumSamples =sum(NumSamples), NumNonDetects = sum(NumNonDetects))

```

....
```{r, echo=FALSE, message=FALSE, warning=FALSE}
quarterly_summary_disinfection %>% group_by(SummaryTimePeriod)%>%
  summarise(NuLocations=sum(NumSamplingLocations), NuSamples=sum(NumSamples), 
            Analytes=length(unique(AnalyteCode)), Non_detects=sum(NumNonDetects))


```
Here we only provided the first 10 rows of total 40.

* **Step F -** Then the all resulted 5 tables were aggreagated into one table in conformance with HTG and Data Dictionary 2023
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Now mixing all summary data (5 tables) into one table to easily check it 
# annual_summary_max, annual_summary_mean_8, annual_summary_mean_disinfection,
# quarterly_summary_atr_nitr, quarterly_summary_disinfection
Whole_Summaries <- bind_rows(annual_summary_max, annual_summary_mean_8, 
                             annual_summary_mean_disinfection, quarterly_summary_atr_nitr,
                             quarterly_summary_disinfection)

col_order <- c("PWSIDNumber", "Year", "AnalyteCode", "ConcentrationUnits", "Concentration",
               "DateSampled", "AggregationType", "NumSamplingLocations",
               "SummaryTimePeriod", "NumSamples", "NumNonDetects" )

Whole_Summaries <- Whole_Summaries[, col_order]
write_csv(Whole_Summaries, '/Users/babak.jfard/projects/ETHTracking/Data/Water_Data/Summaries_Calculated_20230505.csv')
```

As the final step, we checked the data against the 8 steps in the Gateway 2022 (Appendix F of HTG)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
percent_no_detect <- Whole_Summaries 
```


