{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Water Data for submission into 2024 Spring Water submission\n",
    "This is a revised notebook of Water_20230406.ipynb for 2024 data call. It validates the related water data, and do the tests on Nebraska Water system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from libraries import general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = general.get_Counties_FIPS('NE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "These are the required libraries for validation. Will later be added into a separate library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attempt to creat Data class models\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from typing import Optional, List, Literal\n",
    "from pydantic import BaseModel, ValidationError, Field, conint, confloat, constr, validator\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "\n",
    "version = pydantic.__version__\n",
    "print(version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['RowIdentifier', 'PWSIDNumber', 'YearAssociatedTo', 'YearPulled',\n",
    "#       'PWSName', 'PrincipalCountyServedFIPS', 'PrincipalCityFeatureID',\n",
    "#       'TotalConnections', 'SystemPopulation', 'PrimarySourceCode', 'Latitude',\n",
    "#       'Longitude', 'LocationDerivationCode']\n",
    "\n",
    "class PWS_Inventory(BaseModel):\n",
    "    RowIdentifier: int\n",
    "    StateFIPSCode: int\n",
    "    PWSIDNumber: constr(regex=r'^NE\\d{7}') #Change NE to represent your state code\n",
    "    \n",
    "    YearAssociatedTo: conint(ge=1999, le=2024) \n",
    "    YearPulled: conint(ge=1999, le=2024)\n",
    "    \n",
    "    PWSName: str #Should it have distinction between Unknows and Not Submitted? or just be blank?\n",
    "    \n",
    "    PrincipalCountyServedFIPS: str\n",
    "\n",
    "    @validator('PrincipalCountyServedFIPS')\n",
    "    def check_PrincipalCountyServedFIPS(cls, v):\n",
    "        allowed_values = counties['fips'].tolist()\n",
    "        if v not in allowed_values:\n",
    "            raise ValueError('PrincipalCountyServedFIPS must be a valid FIPS code')\n",
    "        return v      \n",
    "\n",
    "    PrincipalCityFeatureID: int # ????How to get it from the introduced source?\n",
    "\n",
    "    TotalConnections: conint(ge=1, le=9999999)\n",
    "    SystemPopulation: conint(ge=10, le=99999999)\n",
    "    PrimarySourceCode: Literal['GU', 'GUP', 'GW', 'GWP', 'SW', 'SWP', 'U', 'NS']\n",
    "\n",
    "    # For Nebraska in NAD83\n",
    "    Latitude: confloat(ge= 39.999998, le=43.001702) \n",
    "    Longitude: confloat(ge= -104.053514, le=-95.308290)\n",
    "    LocationDerivationCode: Literal['SA', 'MFL', 'PCS', 'GSH','O', '-999', '-888']\n",
    "\n",
    "        \n",
    "# ['RowIdentifier', 'PWSIDNumber', 'Year', 'AnalyteCode', 'DateSampled',\n",
    "#        'AggregationType', 'NumSamplingLocations', 'SummaryTimePeriod',\n",
    "#        'NumSamples', 'NumNonDetects', 'ConcentrationUnits', 'Concentration']\n",
    "class Sampling_Summary(BaseModel):\n",
    "    RowIdentifier: int\n",
    "    PWSIDNumber: constr(regex=r'^NE\\d{7}') #for Nebraska\n",
    "\n",
    "    Year: conint(ge=1999, le=2024)\n",
    "    \n",
    "    \n",
    "    AnalyteCode: Literal['1005', '2050', '2456', '2950', '2039', '1038', '1040', '2987', \n",
    "    '2984', '4010', '4006']\n",
    "    ConcentrationUnits: Literal['ug/l', 'mg/l','pci/l'] # TODO: Apply the rules of what Analyte each applies to\n",
    "    Concentration: float\n",
    "\n",
    "    DateSampled: datetime.date #validate to be from 1/1/1999 to the latest complete year\n",
    "\n",
    "    AggregationType: Literal['X', 'MX']\n",
    "    NumSamplingLocations: conint(ge=1, le=9999) #TODO: '-888' for Not Submitted\n",
    "    SummaryTimePeriod: str #TODO: look into its Data Dictionary\n",
    "    NumSamples: int\n",
    "    NumNonDetects: int\n",
    "\n",
    "\n",
    "\n",
    "class Sampling(BaseModel):\n",
    "    RowIdentifier: int\n",
    "    PWSIDNumber: constr(regex=r'^NE\\d{7}') #for Nebraska\n",
    "\n",
    "    Year: conint(ge=1999, le=2024)\n",
    "    \n",
    "    \n",
    "    AnalyteCode: Literal['1005', '2050', '2456', '2950', '2039','1038', '1040', '2987', \n",
    "    '2984', '4010', '4006']\n",
    "    # check if ConcentrationUnits is one of the strings in this list, make the list case insensitive\n",
    "    ConcentrationUnits: Literal['ug/l', 'mg/l','pci/l'] # TODO: Apply the rules of what Analyte each applies to\n",
    "\n",
    "    # check if Concentration is a float and is greater than or equal to 0\n",
    "    Concentration: confloat(ge=0.0)\n",
    "\n",
    "    DateSampled: datetime.date #validate to be from 1/1/1999 to the latest complete year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the PWS_Inventory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory = pd.read_excel('/Users/babak.jfard/projects/EPHTracking/Data/Water_Data_2024/PWSInventory2022_2023.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.PWSIDNumber.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "inventory[inventory.duplicated(subset=['PWSIDNumber', 'YearAssociatedTo'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inventory.columns = inventory.columns.str.replace('\\ufeff', '')\n",
    "\n",
    "#Change the names of several columns to match the names in the validator\n",
    "inventory.rename(columns={'PrincipalCountyServed FIPS': 'PrincipalCountyServedFIPS', '\\ufeffPrincipalCityFeatureId': 'PrincipalCityFeatureID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a uique Identifier, as first column, for each row\n",
    "inventory.insert(0, 'RowIdentifier', inventory.index)\n",
    "#inventory['RowIdentifier'] = inventory.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.to_csv('/Users/babak.jfard/projects/EPHTracking/Data/Water_Data_2024/PWSInventory_latest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_column = list(set(inventory.columns) - set((PWS_Inventory.__fields__.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(PWS_Inventory.__fields__.keys()) - set(inventory.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.Horiz_Ref_Datum.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.drop(columns=rm_column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2024\n",
    "inventory.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is the column names for 2023 data call\n",
    "# inventory.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWS_Inventory.__fields__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the validation for each row as a PWS_Inventory object\n",
    "valid_rows = []\n",
    "# Creat a dictionary that contains the RowIdentifier of the invalid rows and the error message\n",
    "invalid_rows = {}\n",
    "for index, row in inventory.iterrows():\n",
    "    \n",
    "    try:\n",
    "        PWS_Inventory(**row)\n",
    "        # If passeed, add RowIdentifier into valid_rows list\n",
    "        valid_rows.append(row['RowIdentifier'])\n",
    "\n",
    "    except ValidationError as e:\n",
    "        # If failed, add RowIdentifier and the error message into invalid_rows dictionary\n",
    "        invalid_rows[row['RowIdentifier']] = e.errors\n",
    "\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the errorous rows of the inventory dataframe from key values in invalid_rows dictionary\n",
    "errorous_rows = inventory[inventory['RowIdentifier'].isin(invalid_rows.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorous_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorous_rows.to_csv('/Users/babak.jfard/projects/EPHTracking/Data/Water_Data_2024/PWSInventory_2024_errors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Sampling\n",
    "This is the latest file (The unaggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sampling = pd.read_excel('/Users/babak.jfard/projects/EPHTracking/Data/Water_Data_2024/PWSSampleResults2022-2023.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates, and add them into a separate dataframe\n",
    "\n",
    "duplicates = sampling[sampling.duplicated(subset=['PWSIDNumber', 'Year', 'AnalyteCode', 'DateSampled', 'SamplePointID'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.to_excel('/Users/babak.jfard/projects/EPHTracking/Data/Water_Data_2024/duplicates_samples.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sampling.__fields__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling.AnalyteCode.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows for each year, adding heading to the output\n",
    "sampling.Year.value_counts().to_frame('Number of Rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For AnalyteCode Replace all 1038 values with 1040\n",
    "#sampling['AnalyteCode'] = sampling['AnalyteCode'].replace(1038, 1040)\n",
    "\n",
    "# Delete all rows with 1041 as AnalyteCode, which are only NITRITE tests\n",
    "sampling = sampling[sampling['AnalyteCode'] != 1041] #Contained only 17 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling['ConcentrationUnits'] = sampling['ConcentrationUnits'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_cols = list(set(sampling.columns) - set(Sampling.__fields__.keys()))\n",
    "\n",
    "# Remove the columns that are not in the Sampling validator\n",
    "sampling_validation = sampling.drop(columns=del_cols)\n",
    "\n",
    "sampling_validation.insert(0, 'RowIdentifier', sampling.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampling_validation.columns)\n",
    "print(Sampling.__fields__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change type of colum AnlyteCode to string\n",
    "sampling_validation['AnalyteCode'] = sampling_validation['AnalyteCode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now validating the sampling dataframe\n",
    "# Doing the validation for each row as a PWS_Inventory object\n",
    "valid_rows_sampling = []\n",
    "# Creat a dictionary that contains the RowIdentifier of the invalid rows and the error message\n",
    "invalid_rows_sampling = {}\n",
    "for index, row in sampling_validation.iterrows():\n",
    "    \n",
    "    try:\n",
    "        Sampling(**row)\n",
    "        # If passeed, add RowIdentifier into valid_rows list\n",
    "        valid_rows_sampling.append(row['RowIdentifier'])\n",
    "\n",
    "    except ValidationError as e:\n",
    "        # If failed, add RowIdentifier and the error message into invalid_rows dictionary\n",
    "        invalid_rows_sampling[row['RowIdentifier']] = e.errors\n",
    "\n",
    "        #print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling.to_csv('/Users/babak.jfard/projects/EPHTracking/Data/Water_Data_2024/Sample_results_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like there are more undefined AnalyteCodes in the sampling dataframe\n",
    "# Let's see what they are\n",
    "sampling.AnalyteCode.value_counts().to_frame(\"Number of rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are allowable AnalyteCodes as defined in the Sampling class\n",
    "Sampling.__fields__['AnalyteCode'].type_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Aggregate into Sampling Results\n",
    "\n",
    "Each community water system:\n",
    "* annual mean and maz concentration of:\n",
    "\n",
    "--- arsenic, disinfection byproducts (HAA5 and TTHM), \n",
    "\n",
    "--- nitrates, \n",
    "\n",
    "--- atrazine, \n",
    "\n",
    "--- di(2-ethylhexyl) phthalate (DEHP), \n",
    "\n",
    "--- radium, \n",
    "\n",
    "--- tetrachloroethene (tetrachloroethylene) (PCE), \n",
    "\n",
    "--- trichloroethene (trichloroethylene) (TCE), and \n",
    "\n",
    "--- uranium\n",
    "\n",
    "\n",
    "* Mean concentration per quarter \n",
    "\n",
    "--- Nitrate\n",
    "\n",
    "---- Atrazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(inventory.PWSIDNumber.unique()) - set(sampling.PWSIDNumber.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
